<!DOCTYPE html>
<html lang="en-us">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <link href='bootstrap.min.css' rel='stylesheet' type='text/css'>
        <title>Position Embeddings - Andrew Szot</title><meta name="Description" content=""><meta property="og:url" content="http://localhost:1313/posts/position_embeddings/">
  <meta property="og:site_name" content="Andrew Szot">
  <meta property="og:title" content="Position Embeddings">
  <meta property="og:description" content="Intro Transformer models need position embeddings to distinguish the ordering of the inputs. The choice of position embedding is important for a transformer to process longer sequences of inputs than it was trained on. This post explores several position popular embedding methods. The implementations are minimal, self-contained and in pure PyTorch. Experiments use the TinyStories dataset, meaning models only take a couple hours to train from scratch on a consumer grade GPU.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-07-24T00:00:00+00:00">
<meta name="twitter:card" content="summary"><meta name="twitter:title" content="Position Embeddings">
<meta name="twitter:description" content="Intro Transformer models need position embeddings to distinguish the ordering of the inputs. The choice of position embedding is important for a transformer to process longer sequences of inputs than it was trained on. This post explores several position popular embedding methods. The implementations are minimal, self-contained and in pure PyTorch. Experiments use the TinyStories dataset, meaning models only take a couple hours to train from scratch on a consumer grade GPU.">
<meta name="application-name" content="Andrew Szot">
<meta name="apple-mobile-web-app-title" content="Andrew Szot"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/posts/position_embeddings/" /><link rel="prev" href="http://localhost:1313/posts/kv_cache/" /><link rel="next" href="http://localhost:1313/posts/training_parallelism/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Position Embeddings",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/posts\/position_embeddings\/"
        },"genre": "posts","wordcount":  2048 ,
        "url": "http:\/\/localhost:1313\/posts\/position_embeddings\/","datePublished": "2024-07-24T00:00:00+00:00","dateModified": "2024-07-24T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"description": ""
    }
    </script></head>
    <body header-desktop="" header-mobile=""><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : '' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper wrap-pad">
    <div class="title-header content">
      <div class="inner" style='display: flex'>
        <a href='/' class='title-link'>
          <h1>Andrew Szot</h1>
        </a>
        <img id='me-picture' src='/landing/me.jpg'>
      </div>
      <div class='side-btns'>
        <a href='/landing/personal_cv.pdf' class="">CV</a>
        <a href='https://scholar.google.com/citations?hl=en&user=IwIWKPYAAAAJ' class="">Scholar</a>
        <a href='https://github.com/ASzot' class="">GitHub</a>
        <a href='/posts' class="">Posts</a>
        <a href="javascript:void(0);" class="menu-item theme-switch" title="">
          <i class="fas fa-adjust fa-fw"></i>
        </a>
      </div>
    </div>
  </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="inner" style='display: flex'>
              <a href='/'>
                <h1>Andrew Szot</h1>
              </a>
              <img id='me-picture' src='/landing/me.jpg'>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/landing/personal_cv.pdf" title="CV">CV</a><a class="menu-item" href="https://github.com/aszot" title="" rel="noopener noreffer" target="_blank">GitHub</a><a class="menu-item" href="/posts" title="Posts">Posts</a><a class="menu-item" href="https://scholar.google.com/citations?hl=en&amp;user=IwIWKPYAAAAJ" title="Scholar" rel="noopener noreffer" target="_blank">Scholar</a><a href="javascript:void(0);" class="menu-item theme-switch" title="">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main"><div class="toc" id="toc-auto">
            <h2 class="toc-title"></h2>
            <div class="toc-content always-active" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Position Embeddings</h1><div class="post-meta">
            <div class="post-meta-line"></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2024-07-24">2024-07-24</time>&nbsp;</div>
        </div><div class="details toc open" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Content</span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#general-framework">General Framework</a></li>
    <li><a href="#absolute-position-embedding">Absolute Position Embedding</a></li>
    <li><a href="#sinusoidal-position-embedding">Sinusoidal Position Embedding</a></li>
    <li><a href="#attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</a></li>
    <li><a href="#rotary-position-embedding-rope">Rotary Position Embedding (RoPE)</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="intro">Intro</h2>
<p>Transformer models need position embeddings to distinguish the ordering of the inputs. The choice of position embedding is important for a transformer to process longer sequences of inputs than it was trained on. This post explores several position popular embedding methods. The implementations are minimal, self-contained and in pure PyTorch. Experiments use the <a href="https://huggingface.co/datasets/roneneldan/TinyStories" target="_blank" rel="noopener noreffer">TinyStories</a> dataset, meaning models only take a couple hours to train from scratch on a consumer grade GPU. The code is at <a href="https://github.com/ASzot/position-embeddings" target="_blank" rel="noopener noreffer">github.com/aszot/position-embeddings</a> and is only a small single Python file.</p>
<h2 id="general-framework">General Framework</h2>
<p>This section formally defines the transformer layer and how position embeddings modify it.</p>
<p>Let $X \in \mathbb{R}^{N \times D} $ be the input embeddings, $ W^{Q}, W^{K}, W^{V} $ be the $ D \times D$ projection matrices for the query, key, value and output respectively and $ \text{FFN}$ to be a 2-layer MLP. Then a standard transformer layer computes:
$$
X_{\text{ln}} = \text{LayerNorm}(X), K = X_{\text{ln}} W^K, Q = X_{\text{ln}} W^Q, V = X_{\text{ln}} W^V \\
X_{\text{Att}} = X_{\text{ln}} + \text{Softmax} \left( \frac{QK^\top}{\sqrt{D}} \right) V \\
\text{Output} = X_{\text{Att}} + \text{FFN} \left( \text{LayerNorm}\left( X_{\text{Att}} \right)  \right)
$$
The position embedding methods this post explores inject position into the transformer layer in one of two ways:</p>
<ol>
<li>
<p><strong>Input Embeddings:</strong> A $N \times D$ tensor is added to only the first transformer layer to incorporate position information.</p>
</li>
<li>
<p><strong>Modifying Attention</strong>: The $\text{Softmax} \left( \frac{QK^\top}{\sqrt{D}} \right)$ is modified to incorporate position information, typically through modeling some pairwise relation between elements in the input sequence.</p>
</li>
</ol>
<p>These position embeddings can be summarized in the following interface:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_attn_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Return attention mask used for attention.
</span></span></span><span class="line"><span class="cl"><span class="s2">        :returns: Broadcastable to shape [batch_size, num_heads, context_len,
</span></span></span><span class="line"><span class="cl"><span class="s2">            context_len].
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># By default get a causal attention mask.</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">ctx_len</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_pos_embed_offset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        :params idxs: Shape [batch_size, context_len] tensor of indices of the
</span></span></span><span class="line"><span class="cl"><span class="s2">            transformer input.
</span></span></span><span class="line"><span class="cl"><span class="s2">        :returns: Shape [batch_size, context_len, embed_dim] tensor to be added to
</span></span></span><span class="line"><span class="cl"><span class="s2">            the input embeddings tensors.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">modify_qk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_or_v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Modify the `Q` or `V` tensors.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">q_or_v</span>
</span></span></code></pre></div><p>This interface is used in the following transformer forward pass.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transformer_forward</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span> <span class="n">input_tokens</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param input_tokens: Shape [batch_size, context_len] input tokens IDs.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :returns: Shape [batch_size, context_len, #tokens] prediction over next
</span></span></span><span class="line"><span class="cl"><span class="s2">        token probabilities.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ctx_len</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Embed input IDs</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tok_embed</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># `pos_embed` is shape [ctx_len, hidden_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">pos_embed</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">position_embedder</span><span class="o">.</span><span class="n">get_pos_embed_offset</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">idxs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">pos_embed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Include positional information.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">pos_embed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># `attn_mask` is shape [1, #heads (or 1), ctx_len, ctx_len]</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">position_embedder</span><span class="o">.</span><span class="n">get_attn_mask</span><span class="p">(</span><span class="n">ctx_len</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">att_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Convert Q,K,V to shape [batch_size, #heads, ctx_len, embed_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&#34;b t (h v) -&gt; b h t v&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Possibly incorporate position information.</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">position_embedder</span><span class="o">.</span><span class="n">modify_qk</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">position_embedder</span><span class="o">.</span><span class="n">modify_qk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Apply attention. Unless changed by the position embedding,</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `attn_mask` is causal.</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Final shape output of attention operator is</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [batch_size, ctx_len, (#heads * embed_dim)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&#34;b h t d -&gt; b t (h d)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">att_out_proj</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Att-block residual connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Apply FFN</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ffn_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Predict distribution over next tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">output_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span></code></pre></div><h2 id="absolute-position-embedding">Absolute Position Embedding</h2>
<p>In <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreffer">&ldquo;Attention Is All You Need&rdquo;</a>, absolute position embeddings are the simplest position embedding approach where embeddings are learned end-to-end for each context length position. Specifically, the input embedding to the first transformer layer $X^0 \in \mathbb{R}^{N \times D}$ is modified via $ X + E$ where $ E \in \mathbb{R}^{N \times D}$ is a randomly initialized learned tensor. $E$ is learned end-to-end with the rest of the transformer. Using the <code>PositionEmbedding</code> interface, this is implemented as:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AbsoluteEmbedding</span><span class="p">(</span><span class="n">PositionEmbedding</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_context_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_context_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_pos_embed_offset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
</span></span></code></pre></div><p>The below plot shows the training loss when using the absolute position embedding to train a language model from scratch on the TinyStories dataset. The model is a <a href="https://arxiv.org/abs/2307.09288" target="_blank" rel="noopener noreffer">Llama-2</a> style transformer, meaning using RMSNorm and SwiGLU activations. The model has 6 layers 384 embedding dimension and is trained for 1 epoch over the dataset. Most importantly for this investigation, the model is <strong>trained with context length 256</strong>.</p>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/abs_train.png" title="/img/position_embeddings/abs_train.png" data-thumbnail="/img/position_embeddings/abs_train.png" data-sub-html="<h2>Train loss with absolute position embedding.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/abs_train.png"
            data-srcset="/img/position_embeddings/abs_train.png, /img/position_embeddings/abs_train.png 1.5x, /img/position_embeddings/abs_train.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/abs_train.png" width="600px" />
    </a><figcaption class="image-caption">Train loss with absolute position embedding.</figcaption>
    </figure>
<p>While the model can learn and operate over sequences of length less than $ 256$ it has no hope of extrapolating beyond 256 tokens because $E_n$ is still random for $n &gt; 256$. The other position embedding methods try to give the model some extrapolation capabilities for beyond the training sequence length.</p>
<h2 id="sinusoidal-position-embedding">Sinusoidal Position Embedding</h2>
<p>Also in <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreffer">&ldquo;Attention Is All You Need&rdquo;</a>, the sinusoidal position embedding combines the input embedding with a fixed offset computed with sine or cosine. The input embedding to the first transformer layer $X^0 \in \mathbb{R}^{N \times D}$ is again modified via $ X + E$ where $E \in \mathbb{R}^{N \times D}$ is a fixed matrix with, $E_{n,d} = \sin \left( \frac{n}{10,000} \right)^{ \frac{2d}{D}} $ for even $ d$ and $ \cos$ instead of $ \sin $ for odd $ d$. However, unlike the absolute position embedding, this matrix $E$ is fixed throughout training.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SinusoidalEmbedding</span><span class="p">(</span><span class="n">PositionEmbedding</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_context_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># The position embedding is constant and not learnable.</span>
</span></span><span class="line"><span class="cl">        <span class="n">pos_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_context_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `pos_id` is shape [max_context_len, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">pos_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_context_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `pos_multiplier` is shape [embed_dim / 2,]</span>
</span></span><span class="line"><span class="cl">        <span class="n">pos_multiplier</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span> <span class="o">/</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Even embedding indices. Compute an outer product to produce shape</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (max_context_len, embed_dim/2)</span>
</span></span><span class="line"><span class="cl">        <span class="n">pos_embeds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">pos_id</span> <span class="o">*</span> <span class="n">pos_multiplier</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Odd embedding indices</span>
</span></span><span class="line"><span class="cl">        <span class="n">pos_embeds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">pos_id</span> <span class="o">*</span> <span class="n">pos_multiplier</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;pos_embeds&#34;</span><span class="p">,</span> <span class="n">pos_embeds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_pos_embed_offset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idxs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embeds</span><span class="p">[:</span> <span class="n">idxs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span></code></pre></div><p>The below plot shows the training loss when using this position embedding to train a language model in the same setting as above.</p>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/sin_train.png" title="/img/position_embeddings/sin_train.png" data-thumbnail="/img/position_embeddings/sin_train.png" data-sub-html="<h2>Train loss with sinusoidal position embedding.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/sin_train.png"
            data-srcset="/img/position_embeddings/sin_train.png, /img/position_embeddings/sin_train.png 1.5x, /img/position_embeddings/sin_train.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/sin_train.png" width="600px" />
    </a><figcaption class="image-caption">Train loss with sinusoidal position embedding.</figcaption>
    </figure>
<p>The hope is that the model will extrapolate to $E_{n}$ for $n $ greater than the training context length of 256. The following figure shows the loss on <em>train tokens</em> for only the elements in context position between 256 and 512. Note that this is on the same training text, and thus only shows length extrapolation abilities. As the figure shows, the loss is very high for these longer sequences, meaning the model fails to extrapolate to these sequences with sinusoidal position embeddings.</p>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/sin_ext.png" title="/img/position_embeddings/sin_ext.png" data-thumbnail="/img/position_embeddings/sin_ext.png" data-sub-html="<h2>Extrapolation to longer sequences with sinusoidal position embeddings with the plot displaying average loss on token positions 256-512.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/sin_ext.png"
            data-srcset="/img/position_embeddings/sin_ext.png, /img/position_embeddings/sin_ext.png 1.5x, /img/position_embeddings/sin_ext.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/sin_ext.png" width="600px" />
    </a><figcaption class="image-caption">Extrapolation to longer sequences with sinusoidal position embeddings with the plot displaying average loss on token positions 256-512.</figcaption>
    </figure>
<h2 id="attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</h2>
<p><a href="https://arxiv.org/abs/2108.12409" target="_blank" rel="noopener noreffer">Attention with Linear Biases (ALiBi)</a> position embeddings seeks to enable context length extrapolation by adding a pairwise distance penalty in the attention computation. Specifically, the $N \times N$ attention scores $ QK^\top$ are modified by adding a fixed $N \times N$ offset tensor. Specifically, for the softmax input, we compute $QK^\top + E \cdot m$ where $E_{ij} = |i - j|$ when $ i &gt; j$ or 0 otherwise (upper right triangle is all zeros, diagonal is all 0 and lower left triangle are relative distances) and $m $ is a per-attention head scaling factor. We can combine this operation with the causal attention mask which sets the upper right triangle of $ E$ to $ -\infty $. Like sinusoidal embeddings, ALiBi has no learned parameters. But unlike sinusoidal embeddings, AliBi is incorporated in every transformer layer, not just the input. AliBi also encodes relative position between sequence indices, rather than the absolute sequence index. In code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ALiBiEmbedding</span><span class="p">(</span><span class="n">PositionEmbedding</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">max_context_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `index_distance` is shape [max_context_len, max_context_len]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `index_distance[i,j] = abs(j-i)` penalizes elements for being far away.</span>
</span></span><span class="line"><span class="cl">        <span class="n">index_distance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_context_len</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_context_len</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">8</span> <span class="o">/</span> <span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">start</span> <span class="o">*</span> <span class="p">(</span><span class="n">start</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_heads</span><span class="p">)])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `mask` is shape [n_heads, max_context_len, max_context_len]</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">index_distance</span> <span class="o">*</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Make the mask causal by setting any future positions to -inf.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This mask is ultimately ADDED to the QK^T scores, so -inf is</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># needed to set the attention score to -inf before the softmax.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_attn_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">ctx_len</span><span class="p">,</span> <span class="p">:</span><span class="n">ctx_len</span><span class="p">]</span>
</span></span></code></pre></div><p>Training the language model with this position embedding results in extrapolation to longer sequence lengths as shown in the below figure.</p>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/alibi_ext.png" title="/img/position_embeddings/alibi_ext.png" data-thumbnail="/img/position_embeddings/alibi_ext.png" data-sub-html="<h2>Extrapolation to longer sequences with ALiBi position embeddings with the plot displaying average loss on token positions 256-512.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/alibi_ext.png"
            data-srcset="/img/position_embeddings/alibi_ext.png, /img/position_embeddings/alibi_ext.png 1.5x, /img/position_embeddings/alibi_ext.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/alibi_ext.png" width="600px" />
    </a><figcaption class="image-caption">Extrapolation to longer sequences with ALiBi position embeddings with the plot displaying average loss on token positions 256-512.</figcaption>
    </figure>
<h2 id="rotary-position-embedding-rope">Rotary Position Embedding (RoPE)</h2>
<p>Used in many LLMs such as <a href="https://llama.meta.com" target="_blank" rel="noopener noreffer">Llama3</a> and <a href="https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf" target="_blank" rel="noopener noreffer">Gemma</a>, <a href="https://arxiv.org/abs/2104.09864" target="_blank" rel="noopener noreffer">Rotary Position Embedding (RoPE)</a> also relies on relative positions between sequence elements. RoPE is also implemented as a fixed product on the $Q, K$ tensors. The key equation to implement is equation 34 at the bottom of page 7 from the RoPE paper. The <em>rotary matrix</em> $R_{\Theta, m}^d$ encodes relative distances and is set by fixed parameters  $ \Theta = { \theta_1, \dots, \theta_{d/2}}$ where $\theta_i = 10,000^{-2i / d}$. We apply the RoPE transformation by computing $ \left( R_{\Theta, m}^d Q \right) \left( R_{\Theta, m}^d K \right)^\top $ in place of the standard $QK^\top$ in the attention operation. Since $R_{\Theta, m}^d$ is a sparse matrix Eq. 34 of the paper shows we can simplify it&rsquo;s matrix product as:
$$
R_{\Theta, m}^d x =
\begin{pmatrix}
x_1 \cos m \theta_1 \\
x_2 \cos m \theta_1 \\
x_3 \cos m \theta_2 \\
x_4 \cos m \theta_2 \\
\vdots \\
x_{d-1} \cos m \theta_{d/2} \\
x_d \cos m \theta_{d/2}
\end{pmatrix}
+
\begin{pmatrix}
-x_2 \sin m \theta_1 \\
x_1 \sin m \theta_1 \\
-x_4 \sin m \theta_2 \\
x_3 \sin m \theta_2 \\
\vdots \\
-x_{d} \sin m \theta_{d/2} \\
x_{d-1} \sin m \theta_{d/2}
\end{pmatrix}
$$
To implement this, we must compute $\cos m \theta_i$ for all possible sequence indices $m$ and $i \in [0, d/2]$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RoPE_Embedding</span><span class="p">(</span><span class="n">PositionEmbedding</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">max_context_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># From Section 3.2.2 of RoFormer paper.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `theta` shape: [head_embed_dim/2]</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10_000</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">head_embed_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_context_len</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `m_theta` shape: [max_context_len, head_embed_dim//2]</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `sincos_table` shape: [max_context_len, head_embed_dim//2, 2]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Last dimension gives cos or sin of value. This is needed to compute</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Eq. 34 of the RoFormer paper.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sincos_table</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">m_theta</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">m_theta</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">modify_qk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">ctx_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">sincos_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sincos_table</span><span class="p">[:</span><span class="n">ctx_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">sincos_table</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">sincos_table</span><span class="p">,</span> <span class="s2">&#34;c d s -&gt; 1 1 c d s&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cos_table</span> <span class="o">=</span> <span class="n">sincos_table</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">sin_table</span> <span class="o">=</span> <span class="n">sincos_table</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x_split</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Note that Eq. 34 is 1 indexed, but the operations below are 0 indexed.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute even indices of the OUTPUT</span>
</span></span><span class="line"><span class="cl">        <span class="n">even_terms</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Even indices of inputs multiplied by cos terms</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_split</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">cos_table</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Odd indices of inputs multiplied by sin terms.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Notice the negative sign in front of the odd terms.</span>
</span></span><span class="line"><span class="cl">            <span class="o">-</span> <span class="n">x_split</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">sin_table</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute odd indices of the OUTPUT</span>
</span></span><span class="line"><span class="cl">        <span class="n">odd_terms</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Odd indices of inputs multiplied by cos</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_split</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">cos_table</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Even indices of inputs multiplied by cos</span>
</span></span><span class="line"><span class="cl">            <span class="o">+</span> <span class="n">x_split</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">sin_table</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Stack the even and odd terms to be ordered 0,1,2,3,... again</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">even_terms</span><span class="p">,</span> <span class="n">odd_terms</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><p>Like Alibi, RoPE also extrapolates to longer sequence lengths as shown in the figure below.
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/rope_ext.png" title="/img/position_embeddings/rope_ext.png" data-thumbnail="/img/position_embeddings/rope_ext.png" data-sub-html="<h2>Loss on context longer sequences of training tokens with RoPE position embeddings. The loss is computed on tokens in context positions 256-512.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/rope_ext.png"
            data-srcset="/img/position_embeddings/rope_ext.png, /img/position_embeddings/rope_ext.png 1.5x, /img/position_embeddings/rope_ext.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/rope_ext.png" width="600px" />
    </a><figcaption class="image-caption">Loss on context longer sequences of training tokens with RoPE position embeddings. The loss is computed on tokens in context positions 256-512.</figcaption>
    </figure></p>
<h2 id="conclusion">Conclusion</h2>
<p>As shown in the figures below, sinusoidal, ALiBi and RoPE, achieve relatively similar performance on the training distribution of context lengths. However, for extended context lengths, sinusoidal fails. ALiBi slightly outperforms RoPE, but by a small margin. Figure 1 of ALiBi <a href="https://arxiv.org/abs/2108.12409" target="_blank" rel="noopener noreffer">paper</a> also shows better extrapolation than RoPE.</p>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/train.png" title="/img/position_embeddings/train.png" data-thumbnail="/img/position_embeddings/train.png" data-sub-html="<h2>Loss with context length 0-512.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/train.png"
            data-srcset="/img/position_embeddings/train.png, /img/position_embeddings/train.png 1.5x, /img/position_embeddings/train.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/train.png" width="600px" />
    </a><figcaption class="image-caption">Loss with context length 0-512.</figcaption>
    </figure>
<figure class="center-image"><a class="lightgallery" href="/img/position_embeddings/ext.png" title="/img/position_embeddings/ext.png" data-thumbnail="/img/position_embeddings/ext.png" data-sub-html="<h2>Extrapolation to longer sequences with the plot displaying average loss on token positions 256-512.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/position_embeddings/ext.png"
            data-srcset="/img/position_embeddings/ext.png, /img/position_embeddings/ext.png 1.5x, /img/position_embeddings/ext.png 2x"
            data-sizes="auto"
            alt="/img/position_embeddings/ext.png" width="600px" />
    </a><figcaption class="image-caption">Extrapolation to longer sequences with the plot displaying average loss on token positions 256-512.</figcaption>
    </figure>
<p>If you have any questions or spot any errors, contact <a href="mailto:asz.post.contact@gmail.com" rel="">asz.post.contact@gmail.com</a>. Join <a href="https://mailchi.mp/30a660245978/add-email" target="_blank" rel="noopener noreffer">the email list here</a> to be notified about new posts.</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span></span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();"></a></span>&nbsp;|&nbsp;<span><a href="/"></a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/kv_cache/" class="prev" rel="prev" title="KV Cache"><i class="fas fa-angle-left fa-fw"></i>KV Cache</a>
            <a href="/posts/training_parallelism/" class="next" rel="next" title="Training Parallelism">Training Parallelism<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i>
                    Andrew Szot<span itemprop="copyrightYear"> 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
