<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <link href='bootstrap.min.css' rel='stylesheet' type='text/css'>
        <title>KV Cache - Andrew Szot</title><meta name="Description" content=""><meta property="og:url" content="https://www.andrewszot.com/posts/kv_cache/">
  <meta property="og:site_name" content="Andrew Szot">
  <meta property="og:title" content="KV Cache">
  <meta property="og:description" content="Intro The Key-Value (KV) cache is used to speed up next token prediction in transformer models. For example, when large language models (LLMs) are generating text, the KV cache stores the model’s activations for previously generated text to efficiently generate the next token. While not used during training, the KV cache is a crucial implementation detail for fast transformer inference. In this post, I go over KV cache implementation details and show that it results in a $20\times$ inference speedup over naive transformer inference.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-06-25T00:00:00+00:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="KV Cache">
  <meta name="twitter:description" content="Intro The Key-Value (KV) cache is used to speed up next token prediction in transformer models. For example, when large language models (LLMs) are generating text, the KV cache stores the model’s activations for previously generated text to efficiently generate the next token. While not used during training, the KV cache is a crucial implementation detail for fast transformer inference. In this post, I go over KV cache implementation details and show that it results in a $20\times$ inference speedup over naive transformer inference.">
<meta name="application-name" content="Andrew Szot">
<meta name="apple-mobile-web-app-title" content="Andrew Szot"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://www.andrewszot.com/posts/kv_cache/" /><link rel="prev" href="https://www.andrewszot.com/posts/on_lfo/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "KV Cache",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/www.andrewszot.com\/posts\/kv_cache\/"
        },"genre": "posts","wordcount":  3078 ,
        "url": "https:\/\/www.andrewszot.com\/posts\/kv_cache\/","datePublished": "2024-06-25T00:00:00+00:00","dateModified": "2024-06-25T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"description": ""
    }
    </script></head>
    <body header-desktop="" header-mobile=""><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : '' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper wrap-pad">
    <div class="title-header content">
      <div class="inner" style='display: flex'>
        <a href='/' class='title-link'>
          <h1>Andrew Szot</h1>
        </a>
        <img id='me-picture' src='/landing/me.jpg'>
      </div>
      <div class='side-btns'>
        <a href='/landing/personal_cv.pdf' class="">CV</a>
        <a href='https://scholar.google.com/citations?hl=en&user=IwIWKPYAAAAJ' class="">Scholar</a>
        <a href="javascript:void(0);" class="menu-item theme-switch" title="">
          <i class="fas fa-adjust fa-fw"></i>
        </a>
      </div>
    </div>
  </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="inner" style='display: flex'>
              <a href='/'>
                <h1>Andrew Szot</h1>
              </a>
              <img id='me-picture' src='/landing/me.jpg'>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/landing/personal_cv.pdf" title="CV">CV</a><a class="menu-item" href="https://github.com/aszot" title="" rel="noopener noreffer" target="_blank">GitHub</a><a class="menu-item" href="https://scholar.google.com/citations?hl=en&amp;user=IwIWKPYAAAAJ" title="Scholar" rel="noopener noreffer" target="_blank">Scholar</a><a href="javascript:void(0);" class="menu-item theme-switch" title="">
                <i class="fas fa-adjust fa-fw"></i>
            </a>
        </div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main"><div class="toc" id="toc-auto">
            <h2 class="toc-title"></h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">KV Cache</h1><div class="post-meta">
            <div class="post-meta-line"></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2024-06-25">2024-06-25</time>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span></span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#kv-cache-formulation">KV Cache Formulation</a></li>
    <li><a href="#model-definition-and-forward-pass">Model Definition and Forward pass</a></li>
    <li><a href="#training">Training</a></li>
    <li><a href="#generation-without-kv-cache">Generation Without KV Cache</a></li>
    <li><a href="#kv-cache---dynamic">KV Cache - Dynamic</a></li>
    <li><a href="#kv-cache---preallocated">KV Cache - Preallocated</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#changes">Changes</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="intro">Intro</h2>
<p>The Key-Value (KV) cache is used to speed up next token prediction in transformer models. For example, when large language models (LLMs) are generating text, the KV cache stores the model&rsquo;s activations for previously generated text to efficiently generate the next token. While not used during training, the KV cache is a crucial implementation detail for fast transformer inference. In this post, I go over KV cache implementation details and show that it results in a $20\times$ inference speedup over naive transformer inference.</p>
<p>The KV cache implementation is self-contained and minimal in only PyTorch. I illustrate the ideas with a small language model I trained on the <a href="https://huggingface.co/datasets/roneneldan/TinyStories" target="_blank" rel="noopener noreffer">TinyStories</a> dataset. The code is at <a href="https://github.com/ASzot/kv-cache" target="_blank" rel="noopener noreffer">github.com/aszot/kv-cache</a> and is only a small single Python file. It also runs all the below examples in just a couple seconds on a consumer grade GPU.</p>
<h2 id="kv-cache-formulation">KV Cache Formulation</h2>
<p>First, this section formalizes the KV Cache implementation. First consider an input sequence of $n$ input tokens ${ c_1, \dots, c_n }$ (like a prompt) and we want to generate a response of $k$ tokens (like an answer to the prompt). These $n$ tokens are embedded into $d$ dimensional vectors $ X_{1:n} \in \mathbb{R}^{n \times d}$. The transformer layer is parameterized by:</p>
<ul>
<li>Query, key and value projections $W^Q, W^K, W^V$ all in $\mathbb{R}^{d \times d}$ where for simplicity we assume the key and value hidden dimension are the same as the embedding dimension and there is only one attention head.</li>
<li>Attention output projection weight $W^O \in \mathbb{R}^{d \times d}$.</li>
<li>Feedforward network (FFN), which is typically a 2-layer MLP.</li>
</ul>
<p>The transformer layer output $T(X_{1:n})$ is computed as as:
$$
\text{Att}(X_{1:n}) = \text{Softmax} \left( \frac{Q_{1:n}K_{1:n}^\top}{\sqrt{d}} \right) V_{1:n}
$$
$$
T(X_{1:n}) = \text{FFN} \left( \text{LayerNorm} \left( X_{1:n} + \text{Att}(\text{LayerNorm}(X_{1:n})) W^O \right)  \right)
$$
Where $Q_{1:n} = X_{1:n}W^Q, K_{1:n} = X_{1:n}W^K, V_{1:n} = X_{1:n}W^V$. Notice that $\text{Att}(X_{1:n}) $ is shape $n \times n$ and $Q_{1:n}, K_{1:n}, V_{1:n}$ are all shape $ n \times d$. $L$ more transformer layers are iteratively applied to get the final activations $X_{1:n}^L$. The next token $ c_{n+1}$ is predicted based on $X_n^L$.</p>
<p>The KV cache is used to predict the <em>next</em> token $c_{n+2}$ by reusing the previous computations. The key insight is only $X_{n+1}^L$ is needed to predict $c_{n+2}$ (and $X_{1:n}^L $ are anyways the exact same as when computing $c_{n+1}$ due to the causal attention). The previously predicted token $c_{n+1}$ is embedded to get $X_{n+1}$ and compute $Q_{n+1}, K_{n+1}, V_{n+1}$ as before. The attention score of $X_{n+1}$ with $X_{1:n}$ is computed as:
$$
\text{Att}(X_{n+1}) = \text{Softmax} \left( \frac{Q_{n+1} [K_{1:n}, K_{n+1}]^\top}{\sqrt{d}} \right) [V_{1:n}, V_{n+1}]
$$
Where $[A, B]$ indicates concatenating the rows of $A$ and $B$. Notice that $\text{Att}(X_{n+1})$ is shape $1 \times n$. $T(X_{n+1})$ is computed and $c_{n+2}$ is predicted from $X_{n+1}^L$.</p>
<p>$K_{1:n}, V_{1:n}$ are the KV cache for generating token $n+2$. The benefits of the KV cache are that $K_{1:n}, V_{1:n}$ are not recomputed and only need to compute the attention relative to the newly predicted token at $n+1$. Next, this idea is implemented in code.</p>
<h2 id="model-definition-and-forward-pass">Model Definition and Forward pass</h2>
<p>We will implement the transformer and KV cache primarily with <code>torch</code>. <code>tiktoken</code> is used for tokenization and <code>einops</code> is used to make the attention operation more readable.</p>
<p>First, define the transformer parameters. The architecture is based on GPT-2. The <code>TransformerBlock</code> module defines a single transformer layer and will be stacked to create the final transformer. Each layer initializes the projections $W^Q, W^K, W^V$ (via <code>qkv_proj</code>) to produce $Q, K, V$ and the output projection $W^O$ (via <code>att_out_proj</code>). The FFN network does not use any bias parameters. The transformer uses learned position embeddings. For better efficiency, the causal attention mask is precomputed based on a defined maximum possible sequence length.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Single transformer layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_ctx_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Projections for all of Q, K, V (so multiply by 3)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">att_out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Dropout layers to help generalization (maybe not necessary)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># No bias for the FFN.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Common to use an expanded hidden dimension.</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Causal self-attention mask (at index `i` only attend to `&lt;i`).</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_ctx_len</span><span class="p">,</span> <span class="n">max_ctx_len</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_ctx_len</span><span class="p">,</span> <span class="n">max_ctx_len</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CausalTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Full transformer model.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_ctx_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Learned position embeddings.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_ctx_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Dropout layer after token and position embeddings.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">input_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Track the number of heads for the forward pass.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="o">*</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_ctx_len</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Final layer to predict next token.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Tie the output layer with the input embedding layer (they will have</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the same weight values).</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embed</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span>
</span></span></code></pre></div><p>Next, compute the &ldquo;parallel&rdquo; transformer forward pass where the attention scores for all input tokens are computed at the same time. This forward pass is used for training. Note that I implement the attention operation to keep the code simple rather than using the built in <a href="https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html" target="_blank" rel="noopener noreffer">PyTorch attention</a>. The next section shows it&rsquo;s inefficient to reuse this code for inference since it recomputes attention scores for every new tokens. The KV cache is the more efficient alternative for inference.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transformer_forward</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">ctx_len</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">pos_embed</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Embed input IDs and include position information.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tok_embed</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="n">pos_embed</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Convert Q,K,V to shape [batch_size, #heads, context_len, embed_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&#34;b t (h v) -&gt; b h t v&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `att` is shape [batch_size, #heads, context_len, context_len]</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">ctx_len</span><span class="p">,</span> <span class="p">:</span><span class="n">ctx_len</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax per row.</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Final shape output of attention operator is</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [batch_size, context_len, (#heads * embed_dim)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&#34;b h t d -&gt; b t (h d)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">att_out_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Include input residual.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Apply FFN</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Final layernorm.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Predict distribution over next tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="training">Training</h2>
<p>In this section, the transformer model defined above is trained from scrach on the <a href="https://huggingface.co/datasets/roneneldan/TinyStories" target="_blank" rel="noopener noreffer">TinyStories</a> dataset. You can also skip this training part and just use the model I trained and included on the <a href="https://github.com/ASzot/kv-cache/blob/main/model.pth" target="_blank" rel="noopener noreffer">github repo</a>. The training code automatically downloads the TinyStories dataset and trains for 1 epoch over the dataset. The model trains on subsequences of 256 tokens, which can span multiple stories.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">max_ctx_len</span> <span class="o">=</span> <span class="mi">256</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&#34;gpt2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">CausalTransformer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">n_vocab</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_heads</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_ctx_len</span><span class="o">=</span><span class="n">max_ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">block</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><p>This model configuration has around 30 million parameters. Training took around 2 hours on a single A40 GPU using a batch size of 128 producing the following loss curve.</p>
<figure class="center-image"><a class="lightgallery" href="/img/kv_cache/loss.png" title="/img/kv_cache/loss.png" data-thumbnail="/img/kv_cache/loss.png" data-sub-html="<h2>Loss curve training the transformer model on the TinyStories dataset for 1 epoch.</h2>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/img/kv_cache/loss.png"
            data-srcset="/img/kv_cache/loss.png, /img/kv_cache/loss.png 1.5x, /img/kv_cache/loss.png 2x"
            data-sizes="auto"
            alt="/img/kv_cache/loss.png" width="600px" />
    </a><figcaption class="image-caption">Loss curve training the transformer model on the TinyStories dataset for 1 epoch.</figcaption>
    </figure>
<p>The final trained model produces reasonable text generations like the following (with the prompt in bold):</p>
<p>&ldquo;<em><strong>Once upon a time, there was</strong> a boy named Tim. Tim loved to play with his toys and run around outside. One day, Tim&rsquo;s mom told him to be careful and not go too far. Tim didn&rsquo;t listen and kept playing with his toys.</em>
<em>As Tim was playing, he saw a big, scary dog. The dog was barking and running away. Tim&rsquo;s mom told him to be careful and not go near the dog. Tim listened to his mom and told her about the dog.</em>
<em>His mom listened and listened to the dog. Tim was happy to have a new friend and played with his toys. From that day on, Tim learned to be careful and not to touch things.</em>&rdquo;</p>
<h2 id="generation-without-kv-cache">Generation Without KV Cache</h2>
<p>The simplest way to do inference is reuse the <code>transformer_forward</code> call to generate each new token. The current sequence of $n$ tokens is input to <code>transformer_forward</code> which outputs $n$ next token probability distributions. The most likely next token from the final hidden activation is selected and then added to the list of tokens, resulting in $n+1$ tokens. This process is repeated to generate the entire response.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_gen_toks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">stop_on_end_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Tokenize the prompt and add a batch dimension of 1.</span>
</span></span><span class="line"><span class="cl">    <span class="n">prefix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Output this text to the stdout.</span>
</span></span><span class="line"><span class="cl">    <span class="n">stream_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Sequence of current generated tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_toks</span> <span class="o">=</span> <span class="n">prefix</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_gen_toks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute the distribution over next tokens for all tokens.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_token_probs</span> <span class="o">=</span> <span class="n">transformer_forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cur_toks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Predict the most likely next token.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_tok</span> <span class="o">=</span> <span class="n">next_token_probs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Add the predicted token to the generated tokens.</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_toks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cur_toks</span><span class="p">,</span> <span class="n">next_tok</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Get the text for this new token.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_s</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">next_tok</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stop_on_end_token</span> <span class="ow">and</span> <span class="n">next_s</span> <span class="o">==</span> <span class="n">END_TOKEN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Stream the newly generated text to stdout.</span>
</span></span><span class="line"><span class="cl">        <span class="n">stream_text</span><span class="p">(</span><span class="n">next_s</span><span class="p">)</span>
</span></span></code></pre></div><p>Time how long it takes to generate 200 tokens.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&#34;Once upon a time, there was &#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This must be less than the maximum sequence length.</span>
</span></span><span class="line"><span class="cl"><span class="n">num_test_tokens</span> <span class="o">=</span> <span class="mi">200</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Time how long generation takes when not using any KV cache.</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_test_tokens</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2"># Tokens Per Second = </span><span class="si">{</span><span class="n">num_test_tokens</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>This achieves <strong>27</strong> tokes per second.</p>
<h2 id="kv-cache---dynamic">KV Cache - Dynamic</h2>
<p>Using a KV cache will significantly improve on the 27 tokens per-second. The KV cache is implemented by providing an alternative forward pass implementation that will be used for text generation. This new forward pass produces the <em>exact same outputs</em> as <code>transformer_forward</code>, but does so more efficiently by reusing the KV activations as described in detail in the previous <a href="#kv-cache-formulation" rel="">KV cache formulation</a> section. The new forward pass returns the next token distributions along with the KV cache.</p>
<p>Inputs <code>input_tokens</code>is a tensor of length $n$ for an $n$ token prompt and then length $1$ for every subsequent call. The index into the position embeddings must also be offset by the number of existing tokens in the KV cache. After processing $k$ tokens, the attention matrix is now rectangular with shape $1 \times (k+1)$ for every call after processing the prompt. The $1$ is because only a single token is processed at a time.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transformer_forward_kv</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span> <span class="n">input_tokens</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">kv_cache</span><span class="p">:</span> <span class="n">Tensor</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="p">,</span> <span class="n">ctx_len</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># `kv_offset` tracks how many elements are already in the KV cache.</span>
</span></span><span class="line"><span class="cl">    <span class="n">kv_offset</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_offset</span> <span class="o">=</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Embed input IDs and include position information. Account for the</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># existing elements in the KV cache.</span>
</span></span><span class="line"><span class="cl">    <span class="n">pos_embed</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">kv_offset</span><span class="p">,</span> <span class="n">kv_offset</span> <span class="o">+</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tok_embed</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="n">pos_embed</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Tensor that stores the KV cache for the new tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Later combine this KV cache for new tokens with the existing `kv_cache`</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># for old tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># KV cache shape [#layers, 2, batch_size, #heads, context_len, embed_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_kv_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 2 because we need to fit both K and V tensors.</span>
</span></span><span class="line"><span class="cl">            <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Batch size 1</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Apply attention and FFN layers.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&#34;b t (h v) -&gt; b h t v&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Save the calculated K, V values.</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Add the previously calculated K,V values for computing the</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># attention.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Join on the sequence dimension.</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `att` is shape [batch_size, num_heads, ctx_len, #total_tokens]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># where #total_tokens refers to the total number of tokens including in</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the KV cache. Note that ctx_len will be 1 except when processing the</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># prompt.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask out the upper triangular part of the matrix for causal self-attention.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Assign to `-inf` so the softmax sets it to 0.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># For the KV cache, we must account that we are currently calculating</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the attention scores for `ctx_len` tokens (which is 1 most of the</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># time) but we have `kv_offset` tokens already in the KV cache.</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">block</span><span class="o">.</span><span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">kv_offset</span> <span class="p">:</span> <span class="n">kv_offset</span> <span class="o">+</span> <span class="n">ctx_len</span><span class="p">,</span> <span class="p">:</span> <span class="n">kv_offset</span> <span class="o">+</span> <span class="n">ctx_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `att` is still shape [batch_size, num_heads, ctx_len, #total_tokens]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax per row.</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&#34;b h t d -&gt; b t (h d)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">att_out_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `y` is `x` after self-attention operator.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Combine the new KV cache tokens with the existing KV cache.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Join on the sequence dimension.</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_kv_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv_cache</span><span class="p">,</span> <span class="n">new_kv_cache</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">new_kv_cache</span>
</span></span></code></pre></div><p>Timing it with the same setup gets <strong>248</strong> tokens per second, a <strong>9.2x</strong> speedup over no KV cache. However, this KV cache implementation uses a <em>dynamically allocated</em> KV cache since the <code>new_kv_cache</code> tensor is recreated on every forward pass call to account for the new token. Reallocating this tensor on every forward pass is inefficient. The next section fixes this issue.</p>
<h2 id="kv-cache---preallocated">KV Cache - Preallocated</h2>
<p>A more efficient KV cache implementation preallocates the KV cache tensor ahead of time. Now the code writes the new $K, V$ tensors to the KV cache tensor in place during each generation step. The attention matrix is now $1 \times N$ where $N$ is the <em>maximum sequence length</em>. The causal attention mask ensures that the token being currently predicted does not attend to future KV cache positions (which also are not yet initialized).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transformer_forward_kv_preallocated</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_tokens</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">kv_cache</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq_idxs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">pos_embed</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param kv_cache: This tensor has shape
</span></span></span><span class="line"><span class="cl"><span class="s2">        (#layers, 2, batch_size, #heads, max_context_length, embed_dim)
</span></span></span><span class="line"><span class="cl"><span class="s2">        Write to this tensor in place.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param seq_idxs: Sequence indices of the current input `idxs`.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Embed input IDs and include position information.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tok_embed</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="n">pos_embed</span><span class="p">[</span><span class="n">seq_idxs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Apply self-attention and feedforward layers.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s2">&#34;b t (h k) -&gt; b h t k&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&#34;b t (h v) -&gt; b h t v&#34;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Write the new KV values into the cache.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `kv_cache` has shape:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [#layers, 2, batch_size, #heads, max_ctx_len, hidden_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">seq_idxs</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">seq_idxs</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Get the K, V values for the current layer.</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">kv_cache</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k,v are shape: [batch_size, #heads, max_context_length, hidden_dim]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `att` is shape [batch_size, #heads, #input tokens, max_context_length]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask out the upper triangular part of the matrix for causal self-attention.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Assign to `-inf` so the softmax sets it to 0.</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">block</span><span class="o">.</span><span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">seq_idxs</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># `att` is still shape [batch_size, #heads, #input tokens, max_context_length]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax per row.</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&#34;b h t d -&gt; b t (h d)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">att_out_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># `y` is `x` after self-attention operator.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">+=</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><p>Timing this approach gets <strong>327</strong> tokens per second a <strong>1.3x</strong> speedup over using the dynamically allocated KV cache. The preallocated KV cache is also compatible with the <code>torch.compile</code> optimization because the input shapes to <code>transformer_forward_kv_preallocated</code> are constant. The below code shows both with and without compiling the forward pass. The forward pass for generating the first token will have a different <code>input_tokens</code> shape than subsequent calls since it must process the prompt. Therefore, the code gives 2 warmup steps to compile the forward function for single token generation.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_text_kv_preallocated</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_gen_toks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">CausalTransformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">should_compile</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">stop_on_end_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">prefix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">stream_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_toks</span> <span class="o">=</span> <span class="n">prefix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Pre-allocate a certain KV cache size.</span>
</span></span><span class="line"><span class="cl">    <span class="n">kv_cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">blocks</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 2 because we need to fit both K and V tensors.</span>
</span></span><span class="line"><span class="cl">            <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Batch size 1</span>
</span></span><span class="line"><span class="cl">            <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">model</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Preallocate all position embeddings.</span>
</span></span><span class="line"><span class="cl">    <span class="n">pos_embed</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_ctx_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">seq_idxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_ctx_len</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">should_compile</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">global</span> <span class="n">transformer_forward_kv_preallocated</span>
</span></span><span class="line"><span class="cl">        <span class="n">forward_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">transformer_forward_kv_preallocated</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">forward_fn</span> <span class="o">=</span> <span class="n">transformer_forward_kv_preallocated</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Track the current starting token sequence index</span>
</span></span><span class="line"><span class="cl">    <span class="n">tok_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 1 step for the prompt (which has a different shape) and 1 step for the</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1st generated token. Then all the shapes are consistent.</span>
</span></span><span class="line"><span class="cl">    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_gen_toks</span> <span class="o">+</span> <span class="n">warmup_steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">warmup_steps</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Only start profiling after `warmup_steps`. </span>
</span></span><span class="line"><span class="cl">            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">ctx_len</span> <span class="o">=</span> <span class="n">cur_toks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">pred</span> <span class="o">=</span> <span class="n">forward_fn</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_toks</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq_idxs</span><span class="p">[</span><span class="n">tok_idx</span> <span class="p">:</span> <span class="n">tok_idx</span> <span class="o">+</span> <span class="n">ctx_len</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">pos_embed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tok_idx</span> <span class="o">+=</span> <span class="n">ctx_len</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Predict the most likely next token.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_tok</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_toks</span> <span class="o">=</span> <span class="n">next_tok</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">next_s</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">next_tok</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stop_on_end_token</span> <span class="ow">and</span> <span class="n">next_s</span> <span class="o">==</span> <span class="n">END_TOKEN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">        <span class="n">stream_text</span><span class="p">(</span><span class="n">next_s</span><span class="p">)</span>
</span></span></code></pre></div><p>Running with torch compile results in <strong>554</strong> tokens per second. This is a <strong>1.7x</strong> speedup over the non-compiled version.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The final KV cache implementation with the preallocatd KV cache and torch compile was almost <strong>21x</strong> faster than the generation code without any KV cache. The numbers per setting are summarized below.</p>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th style="text-align: center">Steps-Per-Second</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>No KV Cache</td>
          <td style="text-align: center">27</td>
      </tr>
      <tr>
          <td>Dynamic KV Cache</td>
          <td style="text-align: center">248</td>
      </tr>
      <tr>
          <td>Preallocated KV cache</td>
          <td style="text-align: center">327</td>
      </tr>
      <tr>
          <td>Preallocated KV cache - with torch compile</td>
          <td style="text-align: center">554</td>
      </tr>
  </tbody>
</table>
<p>All the code for this post is at <a href="https://github.com/ASzot/kv-cache" target="_blank" rel="noopener noreffer">github.com/ASzot/kv-cache</a>. The code is just a short single Python file and only takes a couple seconds to run on a consumer grade GPU.</p>
<p>If you have any questions or spot any errors, contact <a href="mailto:asz.post.contact@gmail.com" rel="">asz.post.contact@gmail.com</a>. Join <a href="https://mailchi.mp/30a660245978/add-email" target="_blank" rel="noopener noreffer">the email list here</a> to be notified about new posts.</p>
<h2 id="changes">Changes</h2>
<ul>
<li>July 8th, 2024: Posted.</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span></span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();"></a></span>&nbsp;|&nbsp;<span><a href="/"></a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/on_lfo/" class="prev" rel="prev" title="Practical Challenges of Imitation Learning from Observation"><i class="fas fa-angle-left fa-fw"></i>Practical Challenges of Imitation Learning from Observation</a></div>
</div>
</article></main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i>
                    Andrew Szot<span itemprop="copyrightYear"> 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
